{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You may need to install [OpenCV](https://pypi.python.org/pypi/opencv-python) and [scikit-video](http://www.scikit-video.org/stable/).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\Anaconda3f\\envs\\myEnv\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "import skvideo.io\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "from keras.models import Sequential,model_from_json\n",
    "from keras.layers.core import Dense, Flatten\n",
    "from keras.optimizers import sgd, Adam\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation, AveragePooling2D,Reshape,BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniProject #3: Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Notations__: $E_p$ is the expectation under probability $p$. Please justify each of your answer and widely comment your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a reinforcement learning algorithm, we modelize each step $t$ as an action $a_t$ obtained from a state $s_t$, i.e. $\\{(a_{t},s_{t})_{t\\leq T}\\}$ having the Markov property. We consider a discount factor $\\gamma \\in [0,1]$ that ensures convergence. The goal is to find among all the policies $\\pi$, one that maximizes the expected reward:\n",
    "\n",
    "\\begin{equation*}\n",
    "R(\\pi)=\\sum_{t\\leq T}E_{p^{\\pi}}[\\gamma^t r(s_{t},a_{t})] \\> ,\n",
    "\\end{equation*}\n",
    "\n",
    "where: \n",
    "\\begin{equation*}p^{\\pi}(a_{0},a_{1},s_{1},...,a_{T},s_{T})=p(a_{0})\\prod_{t=1}^{T}\\pi(a_{t}|s_{t})p(s_{t+1}|s_{t},a_{t}) \\> .\n",
    "\\end{equation*}\n",
    "\n",
    "We note the $Q$-function:\n",
    "\n",
    "\\begin{equation*}Q^\\pi(s,a)=E_{p^{\\pi}}[\\sum_{t\\leq T}\\gamma^{t}r(s_{t},a_{t})|s_{0}=s,a_{0}=a] \\> .\n",
    "\\end{equation*}\n",
    "\n",
    "Thus, the optimal Q function is:\n",
    "\\begin{equation*}\n",
    "Q^*(s,a)=\\max_{\\pi}Q^\\pi(s,a) \\> .\n",
    "\\end{equation*}\n",
    "\n",
    "In this project, we will apply the deep reinforcement learning techniques to a simple game: an agent will have to learn from scratch a policy that will permit it maximizing a reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The environment, the agent and the game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Environment``` is an abstract class that represents the states, rewards, and actions to obtain the new state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def act(self, act):\n",
    "        \"\"\"\n",
    "        One can act on the environment and obtain its reaction:\n",
    "        - the new state\n",
    "        - the reward of the new state\n",
    "        - should we continue the game?\n",
    "\n",
    "        :return: state, reward, game_over\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reinitialize the environment to a random state and returns\n",
    "        the original state\n",
    "\n",
    "        :return: state\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def draw(self):\n",
    "        \"\"\"\n",
    "        Visualize in the console or graphically the current state\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method ```act``` allows to act on the environment at a given state $s_t$ (stored internally), via action $a_t$. The method will return the new state $s_{t+1}$, the reward $r(s_{t},a_{t})$ and determines if $t\\leq T$ (*game_over*).\n",
    "\n",
    "The method ```reset``` simply reinitializes the environment to a random state $s_0$.\n",
    "\n",
    "The method ```draw``` displays the current state $s_t$ (this is useful to check the behavior of the Agent).\n",
    "\n",
    "We modelize $s_t$ as a tensor, while $a_t$ is an integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the ```Agent``` is to interact with the ```Environment``` by proposing actions $a_t$ obtained from a given state $s_t$ to attempt to maximize its __reward__ $r(s_t,a_t)$. We propose the following abstract class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, epsilon=0.1, n_action=4):\n",
    "        self.epsilon = epsilon\n",
    "        self.n_action = n_action\n",
    "    \n",
    "    def set_epsilon(self,e):\n",
    "        self.epsilon = e\n",
    "\n",
    "    def act(self,s,train=True):\n",
    "        \"\"\" This function should return the next action to do:\n",
    "        an integer between 0 and 4 (not included) with a random exploration of epsilon\"\"\"\n",
    "        if train:\n",
    "            if np.random.rand() <= self.epsilon:\n",
    "                a = np.random.randint(0, self.n_action, size=1)[0]\n",
    "            else:\n",
    "                a = self.learned_act(s)\n",
    "        else: # in some cases, this can improve the performance.. remove it if poor performances\n",
    "            a = self.learned_act(s)\n",
    "\n",
    "        return a\n",
    "\n",
    "    def learned_act(self,s):\n",
    "        \"\"\" Act via the policy of the agent, from a given state s\n",
    "        it proposes an action a\"\"\"\n",
    "        pass\n",
    "\n",
    "    def reinforce(self, s, n_s, a, r, game_over_):\n",
    "        \"\"\" This function is the core of the learning algorithm. \n",
    "        It takes as an input the current state s_, the next state n_s_\n",
    "        the action a_ used to move from s_ to n_s_ and the reward r_.\n",
    "        \n",
    "        Its goal is to learn a policy.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\" This function returns basic stats if applicable: the\n",
    "        loss and/or the model\"\"\"\n",
    "        pass\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\" This function allows to restore a model\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "__Question 1__:\n",
    "Explain the function act. Why is ```epsilon``` essential?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Act is used to determine which action the system should go for according to its state, with a probability $\\epsilon$ to choose a random one to optimize the policy.\n",
    "\n",
    "$\\epsilon$ determines the ratio of exploration-exploitation in the problem. \n",
    "A too low $\\epsilon$ means the agent that will not explore policies and exploit bad learnt policies. \n",
    "A too high $\\epsilon$ means the agent will explore policies a large percentage of rounds and thus quickly reach the optimal policy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### The Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```Agent``` and the ```Environment``` work in an interlaced way as in the following (take some time to understand this code as it is the core of the project)\n",
    "\n",
    "```python\n",
    "\n",
    "epoch = 300\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "\n",
    "\n",
    "# Number of won games\n",
    "score = 0\n",
    "loss = 0\n",
    "\n",
    "\n",
    "for e in range(epoch):\n",
    "    # At each epoch, we restart to a fresh game and get the initial state\n",
    "    state = env.reset()\n",
    "    # This assumes that the games will end\n",
    "    game_over = False\n",
    "\n",
    "    win = 0\n",
    "    lose = 0\n",
    "    \n",
    "    while not game_over:\n",
    "        # The agent performs an action\n",
    "        action = agent.act(state)\n",
    "\n",
    "        # Apply an action to the environment, get the next state, the reward\n",
    "        # and if the games end\n",
    "        prev_state = state\n",
    "        state, reward, game_over = env.act(action)\n",
    "\n",
    "        # Update the counters\n",
    "        if reward > 0:\n",
    "            win = win + reward\n",
    "        if reward < 0:\n",
    "            lose = lose -reward\n",
    "\n",
    "        # Apply the reinforcement strategy\n",
    "        loss = agent.reinforce(prev_state, state,  action, reward, game_over)\n",
    "\n",
    "    # Save as a mp4\n",
    "    if e % 10 == 0:\n",
    "        env.draw(e)\n",
    "\n",
    "    # Update stats\n",
    "    score += win-lose\n",
    "\n",
    "    print(\"Epoch {:03d}/{:03d} | Loss {:.4f} | Win/lose count {}/{} ({})\"\n",
    "          .format(e, epoch, loss, win, lose, win-lose))\n",
    "    agent.save()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The game, *eat cheese*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A rat runs on an island and tries to eat as much as possible. The island is subdivided into $N\\times N$ cells, in which there are cheese (+0.5) and poisonous cells (-1). The rat has a visibility of 2 cells (thus it can see $5^2$ cells). The rat is given a time $T$ to accumulate as much food as possible. It can perform 4 actions: going up, down, left, right. \n",
    "\n",
    "The goal is to code an agent to solve this task that will learn by trial and error. We propose the following environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    def __init__(self, grid_size=10, max_time=500, temperature=0.1):\n",
    "        grid_size = grid_size+4\n",
    "        self.grid_size = grid_size\n",
    "        self.max_time = max_time\n",
    "        self.temperature = temperature\n",
    "\n",
    "        #board on which one plays\n",
    "        self.board = np.zeros((grid_size,grid_size))\n",
    "        self.position = np.zeros((grid_size,grid_size))\n",
    "\n",
    "        # coordinate of the cat\n",
    "        self.x = 0\n",
    "        self.y = 1\n",
    "\n",
    "        # self time\n",
    "        self.t = 0\n",
    "\n",
    "        self.scale=16\n",
    "\n",
    "        self.to_draw = np.zeros((max_time+2, grid_size*self.scale, grid_size*self.scale, 3))\n",
    "\n",
    "\n",
    "    def draw(self,e):\n",
    "        skvideo.io.vwrite(str(e) + '.mp4', self.to_draw)\n",
    "\n",
    "    def get_frame(self,t):\n",
    "        b = np.zeros((self.grid_size,self.grid_size,3))+128\n",
    "        b[self.board>0,0] = 256\n",
    "        b[self.board < 0, 2] = 256\n",
    "        b[self.x,self.y,:]=256\n",
    "        b[-2:,:,:]=0\n",
    "        b[:,-2:,:]=0\n",
    "        b[:2,:,:]=0\n",
    "        b[:,:2,:]=0\n",
    "        \n",
    "        b =  cv2.resize(b, None, fx=self.scale, fy=self.scale, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        self.to_draw[t,:,:,:]=b\n",
    "\n",
    "\n",
    "    def act(self, action):\n",
    "        \"\"\"This function returns the new state, reward and decides if the\n",
    "        game ends.\"\"\"\n",
    "\n",
    "        self.get_frame(int(self.t))\n",
    "\n",
    "        self.position = np.zeros((self.grid_size, self.grid_size))\n",
    "\n",
    "        self.position[0:2,:]= -1\n",
    "        self.position[:,0:2] = -1\n",
    "        self.position[-2:, :] = -1\n",
    "        self.position[:, -2:] = -1\n",
    "\n",
    "        self.position[self.x, self.y] = 1\n",
    "        if action == 0:\n",
    "            if self.x == self.grid_size-3:\n",
    "                self.x = self.x-1\n",
    "            else:\n",
    "                self.x = self.x + 1\n",
    "        elif action == 1:\n",
    "            if self.x == 2:\n",
    "                self.x = self.x+1\n",
    "            else:\n",
    "                self.x = self.x-1\n",
    "        elif action == 2:\n",
    "            if self.y == self.grid_size - 3:\n",
    "                self.y = self.y - 1\n",
    "            else:\n",
    "                self.y = self.y + 1\n",
    "        elif action == 3:\n",
    "            if self.y == 2:\n",
    "                self.y = self.y + 1\n",
    "            else:\n",
    "                self.y = self.y - 1\n",
    "        else:\n",
    "            RuntimeError('Error: action not recognized')\n",
    "\n",
    "        self.t = self.t + 1\n",
    "        reward = self.board[self.x, self.y]\n",
    "        self.board[self.x, self.y] = 0\n",
    "        game_over = self.t > self.max_time\n",
    "        state = np.concatenate((self.board.reshape(self.grid_size, self.grid_size,1),\n",
    "                        self.position.reshape(self.grid_size, self.grid_size,1)),axis=2)\n",
    "        state = state[self.x-2:self.x+3,self.y-2:self.y+3,:]\n",
    "\n",
    "        return state, reward, game_over\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"This function resets the game and returns the initial state\"\"\"\n",
    "\n",
    "        self.x = np.random.randint(3, self.grid_size-3, size=1)[0]\n",
    "        self.y = np.random.randint(3, self.grid_size-3, size=1)[0]\n",
    "\n",
    "\n",
    "        bonus = 0.5*np.random.binomial(1,self.temperature,size=self.grid_size**2)\n",
    "        bonus = bonus.reshape(self.grid_size,self.grid_size)\n",
    "\n",
    "        malus = -1.0*np.random.binomial(1,self.temperature,size=self.grid_size**2)\n",
    "        malus = malus.reshape(self.grid_size, self.grid_size)\n",
    "\n",
    "        self.to_draw = np.zeros((self.max_time+2, self.grid_size*self.scale, self.grid_size*self.scale, 3))\n",
    "\n",
    "\n",
    "        malus[bonus>0]=0\n",
    "\n",
    "        self.board = bonus + malus\n",
    "\n",
    "        self.position = np.zeros((self.grid_size, self.grid_size))\n",
    "        self.position[0:2,:]= -1\n",
    "        self.position[:,0:2] = -1\n",
    "        self.position[-2:, :] = -1\n",
    "        self.position[:, -2:] = -1\n",
    "        self.board[self.x,self.y] = 0\n",
    "        self.t = 0\n",
    "\n",
    "        state = np.concatenate((\n",
    "                               self.board.reshape(self.grid_size, self.grid_size,1),\n",
    "                        self.position.reshape(self.grid_size, self.grid_size,1)),axis=2)\n",
    "\n",
    "        state = state[self.x - 2:self.x + 3, self.y - 2:self.y + 3, :]\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following elements are important because they correspond to the hyper parameters for this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "size = 13\n",
    "T=200\n",
    "temperature=0.3\n",
    "epochs_train=10 # set small when debugging\n",
    "epochs_test=10 # set small when debugging\n",
    "\n",
    "# display videos\n",
    "def display_videos(name):\n",
    "    video = io.open(name, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    return '''<video alt=\"test\" controls>\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2__ Explain the use of the arrays ```position``` and ```board```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "position has dimensions `grid_size` $\\times$ `grid_size` with a margin of 2 cells for padding the environment. Borders or margins have a value of -1 qnd are not reachable by the agent, a value of 1 is given to the previous position cell and the other cells have a value of 0.\n",
    "\n",
    "The board is used to show the rewards of the game. board is dimensions `grid_size` $\\times$ `grid_size` with cells that contain cheese have a reward of `0.5` and those that contain poison have a reward of `-1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "__Question 3__ Implement a random Agent (only ```learned_act``` needs to be implemented):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "    def __init__(self):\n",
    "        super(RandomAgent, self).__init__()\n",
    "        pass\n",
    "\n",
    "    def learned_act(self, s):\n",
    "        \n",
    "        action = np.random.randint(0,3) \n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "__Question 4__ Visualize the game moves. You need to fill in the following function for the evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(agent,env,epochs,prefix=''):\n",
    "    # Number of won games\n",
    "    score = 0\n",
    "        \n",
    "    for e in range(epochs):\n",
    "        state = env.reset()\n",
    "        game_over = False\n",
    "        win, lose = 0, 0 # sum of positive and negative rewards\n",
    "        \n",
    "        while not game_over:\n",
    "            action = agent.act(state, train=False)\n",
    "            state, reward, game_over = env.act(action)\n",
    "            if reward > 0:\n",
    "                win += reward\n",
    "            else:\n",
    "                lose -= reward\n",
    "        # Save as a mp4\n",
    "        env.draw(prefix+str(e))\n",
    "\n",
    "        # Update stats\n",
    "        score = score + win-lose\n",
    "\n",
    "        print(\"Win/lose count {}/{}. Average score ({})\"\n",
    "              .format(win, lose, score/(1+e)))\n",
    "    print('Final score: '+str(score/epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win/lose count 7.0/12.0. Average score (-5.0)\n",
      "Win/lose count 2.5/4.0. Average score (-3.25)\n",
      "Win/lose count 3.0/9.0. Average score (-4.166666666666667)\n",
      "Win/lose count 5.0/4.0. Average score (-2.875)\n",
      "Win/lose count 3.5/8.0. Average score (-3.2)\n",
      "Win/lose count 5.5/9.0. Average score (-3.25)\n",
      "Win/lose count 3.0/7.0. Average score (-3.357142857142857)\n",
      "Win/lose count 5.5/8.0. Average score (-3.25)\n",
      "Win/lose count 3.0/8.0. Average score (-3.4444444444444446)\n",
      "Win/lose count 5.0/6.0. Average score (-3.2)\n",
      "Win/lose count 2.5/4.0. Average score (-3.0454545454545454)\n",
      "Win/lose count 3.5/6.0. Average score (-3.0)\n",
      "Win/lose count 4.0/6.0. Average score (-2.923076923076923)\n",
      "Win/lose count 6.0/12.0. Average score (-3.142857142857143)\n",
      "Win/lose count 5.0/2.0. Average score (-2.7333333333333334)\n",
      "Win/lose count 7.5/10.0. Average score (-2.71875)\n",
      "Win/lose count 4.0/5.0. Average score (-2.6176470588235294)\n",
      "Win/lose count 4.0/7.0. Average score (-2.638888888888889)\n",
      "Win/lose count 3.0/8.0. Average score (-2.763157894736842)\n",
      "Win/lose count 7.0/12.0. Average score (-2.875)\n",
      "Final score: -2.875\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls>\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the game\n",
    "env = Environment(grid_size=size, max_time=T,temperature=temperature)\n",
    "\n",
    "# Initialize the agent!\n",
    "agent = RandomAgent()\n",
    "\n",
    "test(agent,env,epochs_test,prefix='random')\n",
    "HTML(display_videos('random0.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume here that $T=\\infty$.\n",
    "\n",
    "***\n",
    "__Question 5__ Let $\\pi$ be a policy, show that:\n",
    "\n",
    "\\begin{equation*}\n",
    "Q^{\\pi}(s,a)=E_{(s',a')\\sim p(.|s,a)}[r(s,a)+\\gamma Q^{\\pi}(s',a')]\n",
    "\\end{equation*}\n",
    "\n",
    "Then, show that for the optimal policy $\\pi^*$ (we assume its existence), the following holds: \n",
    "\n",
    "\\begin{equation*}\n",
    "Q^{*}(s,a)=E_{s'\\sim \\pi^*(.|s,a)}[r(s,a)+\\gamma\\max_{a'}Q^{*}(s',a')].\n",
    "\\end{equation*}\n",
    "Finally, deduce that a plausible objective is:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathcal{L}(\\theta)=E_{s' \\sim \\pi^*(.|s,a)}\\Vert r+\\gamma\\max\\max_{a'}Q(s',a',\\theta)-Q(s,a,\\theta)\\Vert^{2}.\n",
    "\\end{equation*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "\n",
    "We have,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q^{\\pi}(s,a) &= E_{\\pi}[ \\sum_{t=0}^{\\infty}\\gamma^t r(s_t,a_t) \\ | \\ s_0 = s, a_0 = a]\\\\\n",
    "&= E[r(s,a)]+ \\gamma \\sum_{s'\\in S}^{} \\sum_{a' \\in A}^{} p(s_1 = s',a_1 = a'|s_0 = s, a_0 = a)E_{\\pi}[\\sum_{t=1}^{\\infty}\\gamma^{t-1} r(s_{t},a_{t}) \\ |\\  s_1 = s',a_1 = a' ]\\\\\n",
    "& = E[r(s,a)]+ \\gamma \\sum_{s'\\in S}^{} \\sum_{a' \\in A}^{} p(s_1 = s',a_1 = a'|s_0 = s, a_0 = a)Q^{\\pi}(s',a')\\\\\n",
    "&= E_{(s',a')\\sim p(.|s,a)}[r(s,a)+\\gamma Q^{\\pi}(s',a')]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$Q^{*}(s,a) = max_{\\pi} Q^{\\pi}(s,a)$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q^*(s,a) & = \\max_{\\pi}E_{(s',a')\\sim p(.|s,a)}[r(s,a)+\\gamma Q^{\\pi}(s',a')] \\\\\n",
    " & = r(s,a) + \\gamma \\max_{\\pi} [ \\sum_{s' \\in S}^{}  \\sum_{a' \\in A}^{} p(s',a'|s,a)Q^{\\pi}(s',a') ] \\\\\n",
    " & = r(s,a) + \\gamma  \\sum_{s' \\in S}^{}p(s'|s,a)\n",
    " \\max_{\\pi} [\\sum_{a' \\in A}^{} \\pi(a'|s')Q^{\\pi}(s',a') ]\\\\\n",
    "& = E_{s'\\sim \\pi^*(.|s,a)}[r(s,a)+\\gamma\\max_{a'}Q^{*}(s',a')]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Bellman function to be the target of loss function used to learn with the proposed mse loss function is viable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "The DQN-learning algorithm relies on these derivations to train the parameters $\\theta$ of a Deep Neural Network:\n",
    "\n",
    "1. At the state $s_t$, select the action $a_t$ with best reward using $Q_t$ and store the results;\n",
    "\n",
    "2. Obtain the new state $s_{t+1}$ from the environment $p$;\n",
    "\n",
    "3. Store $(s_t,a_t,s_{t+1})$;\n",
    "\n",
    "4. Obtain $Q_{t+1}$ by minimizing  $\\mathcal{L}$ from a recovered batch from the previously stored results.\n",
    "\n",
    "***\n",
    "__Question 6__ Implement the class ```Memory``` that stores moves (in a replay buffer) via ```remember``` and provides a ```random_access``` to these. Specify a maximum memory size to avoid side effects. You can for example use a ```list()``` and set by default ```max_memory=100```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class Memory(object):\n",
    "    def __init__(self, max_memory=100):\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "\n",
    "    def remember(self, m):\n",
    "        self.memory.append(m)\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            self.memory.pop(0) \n",
    "            \n",
    "    def random_access(self):\n",
    "        return random.choice(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "The pipeline we will use for training is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent,env,epoch,prefix=''):\n",
    "    # Number of won games\n",
    "    score = 0\n",
    "    loss = 0\n",
    "\n",
    "    for e in range(epoch):\n",
    "        # At each epoch, we restart to a fresh game and get the initial state\n",
    "        state = env.reset()\n",
    "        # This assumes that the games will terminate\n",
    "        game_over = False\n",
    "\n",
    "        win = 0\n",
    "        lose = 0\n",
    "\n",
    "        while not game_over:\n",
    "            # The agent performs an action\n",
    "            action = agent.act(state)\n",
    "\n",
    "            # Apply an action to the environment, get the next state, the reward\n",
    "            # and if the games end\n",
    "            prev_state = state\n",
    "            state, reward, game_over = env.act(action)\n",
    "\n",
    "            # Update the counters\n",
    "            if reward > 0:\n",
    "                win = win + reward\n",
    "            if reward < 0:\n",
    "                lose = lose -reward\n",
    "\n",
    "            # Apply the reinforcement strategy\n",
    "            loss = agent.reinforce(prev_state, state,  action, reward, game_over)\n",
    "\n",
    "        # Save as a mp4\n",
    "        if e % 10 == 0:\n",
    "            env.draw(prefix+str(e))\n",
    "\n",
    "        # Update stats\n",
    "        score += win-lose\n",
    "\n",
    "        print(\"Epoch {:03d}/{:03d} | Loss {:.4f} | Win/lose count {}/{} ({})\"\n",
    "              .format(e, epoch, loss, win, lose, win-lose))\n",
    "        agent.save(name_weights=prefix+'model.h5',name_model=prefix+'model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.zeros((2, 5,5,3))\n",
    "z[0] = 1\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "__Question 7__ Implement the DQN training algorithm using a cascade of fully connected layers. You can use different learning rate, batch size or memory size parameters. In particular, the loss might oscillate while the player will start to win the games. You have to find a good criterium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(Agent):\n",
    "    def __init__(self, grid_size,  epsilon = 0.1, memory_size=100, batch_size = 16,n_state=2):\n",
    "        super(DQN, self).__init__(epsilon = epsilon)\n",
    "\n",
    "        # Discount for Q learning\n",
    "        self.discount = 0.99\n",
    "        \n",
    "        self.grid_size = grid_size\n",
    "        \n",
    "        # number of state\n",
    "        self.n_state = n_state\n",
    "\n",
    "        # Memory\n",
    "        self.memory = Memory(memory_size)\n",
    "        \n",
    "        # Batch size when learning\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def learned_act(self, s):\n",
    "        predicted_Q = self.model.predict(s.reshape(1,5,5,self.n_state))\n",
    "        return np.argmax(predicted_Q)\n",
    "    \n",
    "    def reinforce(self, s_, n_s_, a_, r_, game_over_):\n",
    "        # Two steps: first memorize the states, second learn from the pool\n",
    "\n",
    "        # First, memorize the states\n",
    "        self.memory.remember([s_, n_s_, a_, r_, game_over_])\n",
    "        \n",
    "        input_states = np.zeros((self.batch_size, 5,5,self.n_state))\n",
    "        \n",
    "        target_q = np.zeros((self.batch_size, 4))\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            s, n_s, a, r, game_over = self.memory.random_access()\n",
    "            prd = self.model.predict(np.array([s, n_s]))\n",
    "            target_q[i] = prd[0]\n",
    "            input_states[i] = s\n",
    "            if game_over_:\n",
    "                target_q[i][a] = r\n",
    "            else:\n",
    "                target_q[i][a] = r + self.discount*max(prd[1])\n",
    "\n",
    "        target_q = np.clip(target_q, -3, 3)\n",
    "\n",
    "        l = self.model.train_on_batch(input_states, target_q)\n",
    "\n",
    "        return l\n",
    "\n",
    "    def save(self,name_weights='model.h5',name_model='model.json'):\n",
    "        self.model.save_weights(name_weights, overwrite=True)\n",
    "        with open(name_model, \"w\") as outfile:\n",
    "            json.dump(self.model.to_json(), outfile)\n",
    "            \n",
    "    def load(self,name_weights='model.h5',name_model='model.json'):\n",
    "        with open(name_model, \"r\") as jfile:\n",
    "            model = model_from_json(json.load(jfile))\n",
    "        model.load_weights(name_weights)\n",
    "        model.compile(\"adam\", \"mse\")\n",
    "        self.model = model\n",
    "\n",
    "            \n",
    "class DQN_FC(DQN):\n",
    "    def __init__(self, *args,**kwargs):\n",
    "        lr = kwargs.pop('lr', 0.1)\n",
    "        super(DQN_FC, self).__init__( *args,**kwargs)\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Flatten(input_shape=(5, 5, self.n_state)))\n",
    "        model.add(Dense(32,activation='tanh'))\n",
    "        model.add(Dense(4))\n",
    "        model.compile(Adam(lr=lr), loss = \"mse\")\n",
    "        \n",
    "        self.model = model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000/010 | Loss 0.7106 | Win/lose count 7.0/5.0 (2.0)\n",
      "Epoch 001/010 | Loss 1.0204 | Win/lose count 9.5/7.0 (2.5)\n",
      "Epoch 002/010 | Loss 1.5643 | Win/lose count 4.5/5.0 (-0.5)\n",
      "Epoch 003/010 | Loss 1.7944 | Win/lose count 3.5/5.0 (-1.5)\n",
      "Epoch 004/010 | Loss 1.6651 | Win/lose count 4.0/4.0 (0.0)\n",
      "Epoch 005/010 | Loss 1.8462 | Win/lose count 4.5/3.0 (1.5)\n",
      "Epoch 006/010 | Loss 1.8671 | Win/lose count 2.0/2.0 (0.0)\n",
      "Epoch 007/010 | Loss 1.9626 | Win/lose count 6.5/3.0 (3.5)\n",
      "Epoch 008/010 | Loss 1.9884 | Win/lose count 5.0/0 (5.0)\n",
      "Epoch 009/010 | Loss 1.9957 | Win/lose count 4.5/1.0 (3.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls>\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Environment(grid_size=size, max_time=T, temperature=0.3)\n",
    "agent = DQN_FC(size, lr=.001, epsilon = 0.1, memory_size=2000, batch_size = 32)\n",
    "train(agent, env, 10, prefix='fc_train')\n",
    "HTML(display_videos('fc_train10.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "__Question 8__ Implement the DQN training algorithm using a CNN (for example, 2 convolutional layers and one final fully connected layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_CNN(DQN):\n",
    "    def __init__(self, *args,**kwargs):\n",
    "\n",
    "        lr = kwargs.pop('lr', 0.1)\n",
    "        print(lr)\n",
    "        super(DQN_CNN, self).__init__(*args,**kwargs)\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, (3,3), activation='relu',input_shape=(5,5,self.n_state)))\n",
    "        model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(4))\n",
    "        model.compile(Adam(lr=lr), loss = \"mse\")\n",
    "        self.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "Epoch 000/010 | Loss 2.0113 | Win/lose count 3.0/3.0 (0.0)\n",
      "Epoch 001/010 | Loss 2.0318 | Win/lose count 8.0/8.0 (0.0)\n",
      "Epoch 002/010 | Loss 1.8443 | Win/lose count 6.5/4.0 (2.5)\n",
      "Epoch 003/010 | Loss 1.7968 | Win/lose count 10.0/10.0 (0.0)\n",
      "Epoch 004/010 | Loss 2.0730 | Win/lose count 5.5/4.0 (1.5)\n",
      "Epoch 005/010 | Loss 1.9105 | Win/lose count 8.0/5.0 (3.0)\n",
      "Epoch 006/010 | Loss 1.9882 | Win/lose count 10.5/1.0 (9.5)\n",
      "Epoch 007/010 | Loss 1.9415 | Win/lose count 5.0/5.0 (0.0)\n",
      "Epoch 008/010 | Loss 1.9767 | Win/lose count 5.0/2.0 (3.0)\n",
      "Epoch 009/010 | Loss 1.9767 | Win/lose count 11.0/1.0 (10.0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls>\n",
       "                <source src=\"data:video/mp4;base64,AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1wNDEAAAAIZnJlZQAAF3FtZGF0AAACrQYF//+p3EXpvebZSLeWLNgg2SPu73gyNjQgLSBjb3JlIDE1NyByMjkzNSA1NDVkZTJmIC0gSC4yNjQvTVBFRy00IEFWQyBjb2RlYyAtIENvcHlsZWZ0IDIwMDMtMjAxOCAtIGh0dHA6Ly93d3cudmlkZW9sYW4ub3JnL3gyNjQuaHRtbCAtIG9wdGlvbnM6IGNhYmFjPTEgcmVmPTMgZGVibG9jaz0xOjA6MCBhbmFseXNlPTB4MToweDExMSBtZT1oZXggc3VibWU9NyBwc3k9MSBwc3lfcmQ9MS4wMDowLjAwIG1peGVkX3JlZj0xIG1lX3JhbmdlPTE2IGNocm9tYV9tZT0xIHRyZWxsaXM9MSA4eDhkY3Q9MCBjcW09MCBkZWFkem9uZT0yMSwxMSBmYXN0X3Bza2lwPTEgY2hyb21hX3FwX29mZnNldD00IHRocmVhZHM9NiBsb29rYWhlYWRfdGhyZWFkcz0xIHNsaWNlZF90aHJlYWRzPTAgbnI9MCBkZWNpbWF0ZT0xIGludGVybGFjZWQ9MCBibHVyYXlfY29tcGF0PTAgY29uc3RyYWluZWRfaW50cmE9MCBiZnJhbWVzPTMgYl9weXJhbWlkPTIgYl9hZGFwdD0xIGJfYmlhcz0wIGRpcmVjdD0xIHdlaWdodGI9MSBvcGVuX2dvcD0wIHdlaWdodHA9MiBrZXlpbnQ9MjUwIGtleWludF9taW49MjUgc2NlbmVjdXQ9NDAgaW50cmFfcmVmcmVzaD0wIHJjX2xvb2thaGVhZD00MCByYz1jcmYgbWJ0cmVlPTEgY3JmPTIzLjAgcWNvbXA9MC42MCBxcG1pbj0wIHFwbWF4PTY5IHFwc3RlcD00IGlwX3JhdGlvPTEuNDAgYXE9MToxLjAwAIAAAAM1ZYiEADP//vaG+BTYUyP+T7/8I/+5H7cfWfrixkIJvrVeQ9GAMl8a/5lGYaXpyc8t7R+vTNAEZz6HlLJIofgUkwDfMsjnE+J8s6FP3lFrFWUnap35Ua3KnnACdVVpEyEQ6jE8HQ88Fb2LlhmSjN2sEarkra1j0vp5+DuUF+4nEBBaBBG/HoQCPEHABbdRX7hRxjZ0Opky1yw6EiqhnyQrbQGzw9PWMqo1HEa0SlFPK50Qqv0NgMsaC+eUZ/s/PLjJr/tOiM46yYSfyiq432+k7MP2LC4u2khUM49ciMPORkcfceWd21IWyMnnHy+gNLvAGem4Ska4En3tB0QIp0t5x2JWI6y4eLJ+R2iOLiTe3szULHIlOwfFpCjCgToTR8NEPOPtdnBBUGewRKZ0QN+PuRHmMoYJFVmTkVbi3XqamE/E7ni8ZvEuBWzAlpz3diYU4vfon6xAtDoDC6U4pagxPd8xPg2pWroO3GuGfBkfLS5O9FgnWnqOvvHeMBpdcbtgQihnTTM2A47CxyaBxI3oCj7Hkm9ECFLsruF38E8MQv6/0VFcP9KR6luVwmixsIk14MmNSRQCdIgZ/eu6aAod2iK9riQDVdKSkNpPjyQHfY9AuGcQLwenaF43EDMVhsPEr5xbntJbK7VQAApWOdErAdg0IMysQInzmrJ3g/kmTho+N5sAJZ/k9NTvcBgXxiQcv8pOB9bzP+Ubx98UtjwxslHIxoQHEcsisKwnCHmB8U2CzQjeeH1BD8X6AWCESB4Sa/GoaCrjrR1HYojGlD7sdx7eyoPdy3UYaEShY+r8gTacgZTjFRNuVQR2E4GCer4JGpE5Q18uDKYGw9deeVkZigdCAhEo8T3VdXwNiUq3RiS/Rpm7cwU/uGpBNivqzhUQ1aK/IFMrOIfZDN817avzOq9PahD34q5EQd4T8YYa0fOEQpluEkLiFSJDDzzCku8R5m2l6cPx+TgIiCsAzD+5I1sG/ZUK7Rhwxeu4LvruNmporL3LFA0OQ3RZma5G8l8qOO2/gGSPdxKAkHnN5toHEY/pxvVoFHYTCQnLEMlAwwHhi5PpAbflhCrRAEuAFt4EcYAAhYEAAAATQZohbEM//p4QARV0tgsnGBU0LgAAABdBmkI8IZMphDf//qeEAC+urSCET/LcAwAAABxBmmVJ4Q8mUwIZ//6eEAC//B8BTPxrMVq7/XGAAAAAEUGeg0URPCv/ACfUo3mhYPzNAAAADgGepGpCvwAnzYxk3JWbAAAAGUGapkmoQWiZTAhn//6eEAEVOEc/hzm+tC8AAAAXQZrHSeEKUmUwIb/+p4QAR1QDvhgRrl8AAAAYQZroSeEOiZTAhv/+p4QARb6OaCtZlNc3AAAAGUGbCUnhDyZTAh3//qmWACI/Hn79kG4qE+AAAAASQZstSeEPJlMCHf/+qZYAAJWBAAAADEGfS0URPC//AACygAAAAA8Bn2p0Qr8AI0qRxHZdlkkAAAAPAZ9sakK/ACSvNEFqPLwfAAAAKUGbcUmoQWiZTAh3//6plgA0Hwo99XcyytUzfgUojz8Cma5ZZQ9OuR25AAAAFUGfj0URLC//ADzJshXAD5iLMhbhPQAAABABn650Qr8ANhJoRPizFHboAAAAEAGfsGpCvwBUFGiZE0rN4sAAAAAaQZu0SahBbJlMCHf//qmWAE4RYboxCOfX/zEAAAAPQZ/SRRUsK/8AfEH/NNPgAAAADwGf82pCvwB7FDdhnqz1QQAAABdBm/hJqEFsmUwId//+qZYAUL5Jfki7oQAAAA5BnhZFFSwv/wBfhFs3oAAAAA8BnjV0Qr8AfxsDQ855dTcAAAAPAZ43akK/AHsUN2GerPVBAAAAE0GaPEmoQWyZTAh3//6plgAAlYAAAAAMQZ5aRRUsL/8AALKBAAAADwGeeXRCvwB/GwNDznl1NwAAAA8BnntqQr8AfvnDRK55dTcAAAAcQZpgSahBbJlMCG///qeEAJt8dPutLM1Nui1tWQAAABBBnp5FFSwv/wBdKBFaUUN0AAAADwGevXRCvwB/GwNdfFqbgAAAABABnr9qQr8AfxXBrjxVtIvhAAAAGUGao0moQWyZTAhv//6nhABnfYPXsz4Ir1MAAAARQZ7BRRUsK/8AVmlG80LB960AAAAOAZ7iakK/AFZbGMm5J1oAAAASQZrnSahBbJlMCGf//p4QAAR9AAAADEGfBUUVLC//AACygQAAAA8BnyR0Qr8AUOyjiOy7Kx8AAAAPAZ8makK/AFQUaILUeXXdAAAAGUGbKEmoQWyZTAhn//6eEAGJ9ffyJEfWEi4AAAAYQZtJSeEKUmUwIZ/+nhAA+Xr7+RIj6wmzAAAAGEGbaknhDomUwIZ//p4QAKR7pvoqVmvhpwAAABlBm4tJ4Q8mUwIb//6nhAAbl1aQQif5bl+AAAAAGUGbrEnhDyZTAhv//qeEABu/YP8JwW6E58AAAAAdQZvOSeEPJlMFETw3//6nhAAR746farzaojIyB90AAAAQAZ/takK/AA6AQCdeAKBdgQAAABtBm+9J4Q8mUwId//6plgADv+2oDm/kcw6QcyEAAAAYQZoTSeEPJlMCHf/+qZYAAZSC3aabjCjwAAAADkGeMUURPC//AAHa/fNgAAAAEAGeUHRCvwAD2KG9l1X8XcEAAAAQAZ5SakK/AAYhK2MIGpNEoAAAABNBmldJqEFomUwId//+qZYAAJWAAAAADEGedUURLC//AACygQAAABABnpR0Qr8ABiLKu76hyF7wAAAAEAGelmpCvwAGIStjCBqTRKEAAAASQZqbSahBbJlMCG///qeEAAEnAAAADEGeuUUVLC//AACygAAAABABnth0Qr8ABiLKu76hyF7xAAAAEAGe2mpCvwAGIStjCBqTRKAAAAAaQZrcSahBbJlMCHf//qmWAAJz8edLOjqeg8EAAAASQZrgSeEKUmUwId/+qZYAAJWBAAAADEGfHkU0TC//AACygAAAABABnz10Qr8AA8Khu6dl2hCAAAAADwGfP2pCvwAD4c4aJXPM3wAAABNBmyRJqEFomUwId//+qZYAAJWAAAAADEGfQkURLC//AACygQAAABABn2F0Qr8AA8Khu6dl2hCAAAAADwGfY2pCvwAD4c4aJXPM3wAAABJBm2hJqEFsmUwIb//+p4QAAScAAAAMQZ+GRRUsL/8AALKBAAAAEAGfpXRCvwADwqG7p2XaEIEAAAAPAZ+nakK/AAPhzholc8zfAAAAG0GbqUmoQWyZTAh3//6plgACY/I6HB0s6Op6FwAAABtBm81J4QpSZTAhv/6nhAAHG9g/zV0FflFCO40AAAAQQZ/rRTRML/8ABDaAg5b4aAAAABABngp0Qr8ABdOgHP60DoCgAAAAEAGeDGpCvwADrKG9itH3bYEAAAAaQZoOSahBaJlMCHf//qmWAAJgUc60PV98xcEAAAASQZoySeEKUmUwId/+qZYAAJWBAAAADEGeUEU0TC//AACygAAAAA8Bnm90Qr8AA+LYGh5zzN8AAAAPAZ5xakK/AAPCobsM9WjpAAAAE0GadkmoQWiZTAh3//6plgAAlYAAAAAMQZ6URREsL/8AALKAAAAADwGes3RCvwAD4tgaHnPM3wAAAA8BnrVqQr8AA8Khuwz1aOkAAAATQZq6SahBbJlMCHf//qmWAACVgQAAAAxBnthFFSwv/wAAsoEAAAAPAZ73dEK/AAPi2Boec8zfAAAADwGe+WpCvwADwqG7DPVo6QAAABNBmv5JqEFsmUwId//+qZYAAJWAAAAADEGfHEUVLC//AACygQAAABABnzt0Qr8AA8Khu6dl2hCBAAAADwGfPWpCvwADwqG7DPVo6QAAABNBmyJJqEFsmUwId//+qZYAAJWAAAAADEGfQEUVLC//AACygQAAABABn390Qr8AA8Khu6dl2hCAAAAADwGfYWpCvwADwqG7DPVo6QAAABNBm2ZJqEFsmUwId//+qZYAAJWAAAAADEGfhEUVLC//AACygQAAABABn6N0Qr8AA8Khu6dl2hCBAAAADwGfpWpCvwADwqG7DPVo6QAAAB1Bm6hJqEFsmUwUTDv//qmWAAOOOoWQk3NPRj9O9wAAABABn8dqQr8ABdLIhNxn1684AAAAGEGbzEnhClJlMCHf/qmWAAOT7S/sWs4khgAAABBBn+pFNEwv/wAENoDl5HghAAAAEAGeCXRCvwAF+eTeVsofAUAAAAAPAZ4LakK/AAPNXzQ60iuAAAAAE0GaEEmoQWiZTAh3//6plgAAlYEAAAAMQZ4uRREsL/8AALKBAAAAEAGeTXRCvwADwqG7p2XaEIEAAAAPAZ5PakK/AAPCobsM9WjpAAAAE0GaVEmoQWyZTAh3//6plgAAlYAAAAAMQZ5yRRUsL/8AALKBAAAAEAGekXRCvwADwqG7p2XaEIAAAAAPAZ6TakK/AAPCobsM9WjpAAAAE0GamEmoQWyZTAh3//6plgAAlYEAAAAMQZ62RRUsL/8AALKAAAAAEAGe1XRCvwADwqG7p2XaEIEAAAAPAZ7XakK/AAPCobsM9WjpAAAAEkGa3EmoQWyZTAhv//6nhAABJwAAAAxBnvpFFSwv/wAAsoEAAAAQAZ8ZdEK/AAPCobunZdoQgAAAAA8BnxtqQr8AA8Khuwz1aOkAAAAZQZsfSahBbJlMCG///qeEAATUfMeRif5cowAAABFBnz1FFSwr/wAD4sxYJCVw0wAAAA4Bn15qQr8AA+LNYrgWzAAAABpBm0BJqEFsmUwIb//+p4QABNvjp9RxoSIWwQAAABpBm2FJ4QpSZTAh3/6plgABlvbUlfC1BP7ImAAAABJBm4VJ4Q6JlMCHf/6plgAAlYEAAAAMQZ+jRRE8L/8AALKAAAAAEAGfwnRCvwADzWKxefwOn8EAAAAQAZ/EakK/AAPMahz/Mt5kQQAAABxBm8lJqEFomUwIb//+p4QABJR81TWbc146fbj5AAAAEEGf50URLC//AALFQIrSkaUAAAAPAZ4GdEK/AAPNYrGEKyHAAAAAEAGeCGpCvwADy84a95pWrcAAAAAaQZoKSahBbJlMCHf//qmWAAJgUc60PV98xcEAAAASQZouSeEKUmUwId/+qZYAAJWAAAAADEGeTEU0TC//AACygAAAAA8Bnmt0Qr8AA+LYGh5zzN8AAAAPAZ5takK/AAPCobsM9WjpAAAAE0GackmoQWiZTAh3//6plgAAlYEAAAAMQZ6QRREsL/8AALKAAAAAEAGer3RCvwADwqG7p2XaEIAAAAAPAZ6xakK/AAPCobsM9WjpAAAAE0GatkmoQWyZTAh3//6plgAAlYAAAAAMQZ7URRUsL/8AALKAAAAAEAGe83RCvwADwqG7p2XaEIEAAAAPAZ71akK/AAPCobsM9WjpAAAAEkGa+kmoQWyZTAhv//6nhAABJwAAAAxBnxhFFSwv/wAAsoEAAAAQAZ83dEK/AAPCobunZdoQgAAAAA8BnzlqQr8AA8Khuwz1aOkAAAASQZs+SahBbJlMCG///qeEAAEnAAAADEGfXEUVLC//AACygQAAABABn3t0Qr8AA8Khu6dl2hCBAAAADwGffWpCvwADwqG7DPVo6QAAABlBm2FJqEFsmUwIZ//+nhAAHEKcc/hzm+x9AAAAEUGfn0UVLCv/AAX52n/RyRZZAAAADgGfoGpCvwAF+dqua9lkAAAAGkGbokmoQWyZTAhv//6nhAALV6J/qt8x+MnBAAAAGEGbw0nhClJlMCG//qeEAAueK0ghE/y3ywAAAB9Bm+VJ4Q6JlMFNEw3//qeEABJR8zU2bbZ9ni2XW6qBAAAAEAGeBGpCvwAO2zwh40NZZoEAAAAfQZoISeEPJlMCG//+p4QAK17dPMssTI7kvc76sT2XVQAAABNBniZFETwr/wAiu0Qy5Y5Zsq+BAAAAEAGeR2pCvwAjrzRMiaVnMcAAAAAcQZpKSahBaJlMFPDP/p4QAPf65dbHDZ+If4x3oAAAABABnmlqQr8ANM7cJuM+vTulAAAAGEGaa0nhClJlMCG//qeEAD++wevZnwRYBwAAABhBmo5J4Q6JlMCGf/6eEAF9kMc/hzm+s58AAAAPQZ6sRRE8K/8AT5twJPJBAAAADQGezWpCvwBPuVh4p5MAAAAZQZrPSahBaJlMCGf//p4QAkpwjn8Oc31mDwAAABhBmvBJ4QpSZTAhn/6eEAOEU45/DnN9ZVsAAAAbQZsRSeEOiZTAhv/+p4QBchhjVAcfoE7/b6ygAAAAF0GbMknhDyZTAhv//qeEA5KGf6dy00kPAAAAGUGbU0nhDyZTAh3//qmWAfPU/KcMfoeIqYAAAAAoQZt3SeEPJlMCHf/+qZYCY9rzHgU1zzW8CmR9Z4FAm/R+57f+0M8UkAAAABBBn5VFETwv/wFv+zw4+jAhAAAADwGftHRCvwHsQkxvUESxaQAAAA8Bn7ZqQr8B7GeaHWhQ2YEAAAAgQZu6SahBaJlMCHf//qmWAlcd0sc93mWWfPt3Xm17iHkAAAASQZ/YRREsK/8B67cgIzss9eMHAAAAEAGf+WpCvwFRsI8lzPkk24EAAAAcQZv+SahBbJlMCHf//qmWAyAqBaJNnvvRj0kJuAAAABBBnhxFFSwv/wGVQEq6xFJBAAAADwGeO3RCvwIfJWmDZLuyVwAAAA8Bnj1qQr8CH2K6yv71YsAAAAASQZoiSahBbJlMCG///qeEAAEnAAAADEGeQEUVLC//AACygQAAABABnn90Qr8CFaieQRWctWLAAAAAEAGeYWpCvwIVqJ45wPt9YsEAAAASQZpmSahBbJlMCGf//p4QAAR8AAAADEGehEUVLC//AACygQAAABABnqN0Qr8CFaieQRWctWLBAAAAEAGepWpCvwIVqJ45wPt9YsEAAAAaQZqpS6hCEFskRggoB/IB/YeAIV/+OEAAEXEAAAAnQZ7HRRUsK/8Cr2PtQcTZ24yYUTHV3zgFppgizb+rNcEHWFMWF8d8AAAAIgGe6GpCvwKvY+1BxOA9PdvVBTQGSaPjzAJA8JSQ4L7I/MAAAAvIbW9vdgAAAGxtdmhkAAAAAAAAAAAAAAAAAAAD6AAAH5AAAQAAAQAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgAACvJ0cmFrAAAAXHRraGQAAAADAAAAAAAAAAAAAAABAAAAAAAAH5AAAAAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAABAAAAAARAAAAEQAAAAAAAkZWR0cwAAABxlbHN0AAAAAAAAAAEAAB+QAAAEAAABAAAAAApqbWRpYQAAACBtZGhkAAAAAAAAAAAAAAAAAAAyAAABlABVxAAAAAAALWhkbHIAAAAAAAAAAHZpZGUAAAAAAAAAAAAAAABWaWRlb0hhbmRsZXIAAAAKFW1pbmYAAAAUdm1oZAAAAAEAAAAAAAAAAAAAACRkaW5mAAAAHGRyZWYAAAAAAAAAAQAAAAx1cmwgAAAAAQAACdVzdGJsAAAAlXN0c2QAAAAAAAAAAQAAAIVhdmMxAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAARABEABIAAAASAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGP//AAAAL2F2Y0MB9AAN/+EAF2f0AA2RmygiEdCAAAADAIAAABkHihTLAQAFaOvjxEgAAAAYc3R0cwAAAAAAAAABAAAAygAAAgAAAAAUc3RzcwAAAAAAAAABAAAAAQAABaBjdHRzAAAAAAAAALIAAAADAAAEAAAAAAEAAAgAAAAAAgAAAgAAAAAEAAAEAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAIAAAAAAIAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACAAAAAACAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAABQAABAAAAAABAAAGAAAAAAEAAAIAAAAAAQAABAAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAEAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAQAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAEAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAYAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACAAAAAACAAACAAAAAAIAAAQAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAQAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAgAAAAAAgAAAgAAAAACAAAEAAAAAAEAAAYAAAAAAQAAAgAAAAABAAAIAAAAAAIAAAIAAAAAAQAABgAAAAABAAACAAAAAAEAAAQAAAAAAQAACAAAAAACAAACAAAAAAUAAAQAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAIAAAAAAIAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACAAAAAACAAACAAAAABxzdHNjAAAAAAAAAAEAAAABAAAAygAAAAEAAAM8c3RzegAAAAAAAAAAAAAAygAABeoAAAAXAAAAGwAAACAAAAAVAAAAEgAAAB0AAAAbAAAAHAAAAB0AAAAWAAAAEAAAABMAAAATAAAALQAAABkAAAAUAAAAFAAAAB4AAAATAAAAEwAAABsAAAASAAAAEwAAABMAAAAXAAAAEAAAABMAAAATAAAAIAAAABQAAAATAAAAFAAAAB0AAAAVAAAAEgAAABYAAAAQAAAAEwAAABMAAAAdAAAAHAAAABwAAAAdAAAAHQAAACEAAAAUAAAAHwAAABwAAAASAAAAFAAAABQAAAAXAAAAEAAAABQAAAAUAAAAFgAAABAAAAAUAAAAFAAAAB4AAAAWAAAAEAAAABQAAAATAAAAFwAAABAAAAAUAAAAEwAAABYAAAAQAAAAFAAAABMAAAAfAAAAHwAAABQAAAAUAAAAFAAAAB4AAAAWAAAAEAAAABMAAAATAAAAFwAAABAAAAATAAAAEwAAABcAAAAQAAAAEwAAABMAAAAXAAAAEAAAABQAAAATAAAAFwAAABAAAAAUAAAAEwAAABcAAAAQAAAAFAAAABMAAAAhAAAAFAAAABwAAAAUAAAAFAAAABMAAAAXAAAAEAAAABQAAAATAAAAFwAAABAAAAAUAAAAEwAAABcAAAAQAAAAFAAAABMAAAAWAAAAEAAAABQAAAATAAAAHQAAABUAAAASAAAAHgAAAB4AAAAWAAAAEAAAABQAAAAUAAAAIAAAABQAAAATAAAAFAAAAB4AAAAWAAAAEAAAABMAAAATAAAAFwAAABAAAAAUAAAAEwAAABcAAAAQAAAAFAAAABMAAAAWAAAAEAAAABQAAAATAAAAFgAAABAAAAAUAAAAEwAAAB0AAAAVAAAAEgAAAB4AAAAcAAAAIwAAABQAAAAjAAAAFwAAABQAAAAgAAAAFAAAABwAAAAcAAAAEwAAABEAAAAdAAAAHAAAAB8AAAAbAAAAHQAAACwAAAAUAAAAEwAAABMAAAAkAAAAFgAAABQAAAAgAAAAFAAAABMAAAATAAAAFgAAABAAAAAUAAAAFAAAABYAAAAQAAAAFAAAABQAAAAeAAAAKwAAACYAAAAUc3RjbwAAAAAAAAABAAAAMAAAAGJ1ZHRhAAAAWm1ldGEAAAAAAAAAIWhkbHIAAAAAAAAAAG1kaXJhcHBsAAAAAAAAAAAAAAAALWlsc3QAAAAlqXRvbwAAAB1kYXRhAAAAAQAAAABMYXZmNTguMjAuMTAw\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Environment(grid_size=size, max_time=T, temperature=0.3)\n",
    "agent = DQN_CNN(size, lr=.01, epsilon = 0.1, memory_size=2000, batch_size = 32)\n",
    "train(agent,env,epochs_train,prefix='cnn_train')\n",
    "HTML(display_videos('cnn_train10.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "__Question 9__ Test both algorithms and compare their performances. Which issue(s) do you observe? Observe also different behaviors by changing the temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "Test of the CNN\n",
      "Win/lose count 2.5/0.0. Average score (2.5)\n",
      "Win/lose count 1.5/0.0. Average score (2.0)\n",
      "Win/lose count 0.5/0.0. Average score (1.5)\n",
      "Win/lose count 3.0/0.0. Average score (1.875)\n",
      "Win/lose count 1.5/0.0. Average score (1.8)\n",
      "Win/lose count 2.5/0.0. Average score (1.9166666666666667)\n",
      "Win/lose count 1.5/1.0. Average score (1.7142857142857142)\n",
      "Win/lose count 1.5/0.0. Average score (1.6875)\n",
      "Win/lose count 1.5/1.0. Average score (1.5555555555555556)\n",
      "Win/lose count 0.5/0.0. Average score (1.45)\n",
      "Final score: 1.45\n",
      "Test of the FC\n",
      "Win/lose count 0.5/1.0. Average score (-0.5)\n",
      "Win/lose count 0.5/0.0. Average score (0.0)\n",
      "Win/lose count 1.5/3.0. Average score (-0.5)\n",
      "Win/lose count 1.0/3.0. Average score (-0.875)\n",
      "Win/lose count 1.0/4.0. Average score (-1.3)\n",
      "Win/lose count 0/2.0. Average score (-1.4166666666666667)\n",
      "Win/lose count 0.5/0.0. Average score (-1.1428571428571428)\n",
      "Win/lose count 0.5/1.0. Average score (-1.0625)\n",
      "Win/lose count 0.5/1.0. Average score (-1.0)\n",
      "Win/lose count 0.5/1.0. Average score (-0.95)\n",
      "Final score: -0.95\n"
     ]
    }
   ],
   "source": [
    "env = Environment(grid_size=size, max_time=T,temperature=0.3)\n",
    "agent_cnn = DQN_CNN(size, lr=.01, epsilon = 0.1, memory_size=2000, batch_size = 32)\n",
    "agent_cnn.load(name_weights='cnn_trainmodel.h5',name_model='cnn_trainmodel.json')\n",
    "\n",
    "agent_fc = DQN_FC(size, lr=.001, epsilon = 0.1, memory_size=2000, batch_size = 32)\n",
    "agent_cnn.load(name_weights='fc_trainmodel.h5',name_model='fc_trainmodel.json')\n",
    "print('Test of the CNN')\n",
    "test(agent_cnn,env,epochs_test,prefix='cnn_test')\n",
    "print('Test of the FC')\n",
    "test(agent_fc,env,epochs_test,prefix='fc_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls>\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(display_videos('cnn_test10.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls>\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(display_videos('fc_test10.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network yields better results than random agent.\n",
    "\n",
    "The CNN uses the location and spatial characteristics whereas the fully connected layer flattens it. \n",
    "\n",
    "Tuning the hyperparameters could continue improve the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "The algorithm tends to not explore the map which can be an issue. We propose two ideas in order to encourage exploration:\n",
    "1. Incorporating a decreasing $\\epsilon$-greedy exploration. You can use the method ```set_epsilon```\n",
    "2. Append via the environment a new state that describes if a cell has been visited or not\n",
    "\n",
    "***\n",
    "__Question 10__ Design a new ```train_explore``` function and environment class ```EnvironmentExploring``` to tackle the issue of exploration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_explore(agent,env,epoch,decay_parameter_epsilon=0.1,prefix=''):\n",
    "    #New training procedure that tries to improve the exploration of the algorithm\n",
    "    #decay_parameter_epsilon in order to use the decreasing $\\epsilon$-greedy exploration\n",
    "    \n",
    "    \n",
    "    # Number of won games\n",
    "    score = 0\n",
    "    loss = 0\n",
    "\n",
    "    for e in range(epoch):\n",
    "        # At each epoch, we restart to a fresh game and get the initial state\n",
    "        state = env.reset()\n",
    "        # This assumes that the games will terminate\n",
    "        game_over = False\n",
    "\n",
    "        win = 0\n",
    "        lose = 0\n",
    "        agent.set_epsilon(agent.epsilon*(1-decay_parameter_epsilon))\n",
    "        while not game_over:\n",
    "            # The agent performs an action\n",
    "            action = agent.act(state)\n",
    "\n",
    "            # Apply an action to the environment, get the next state, the reward\n",
    "            # and if the games end\n",
    "            prev_state = state\n",
    "            state, reward, game_over = env.act(action, train=True)\n",
    "\n",
    "            # Update the counters\n",
    "            if reward > 0:\n",
    "                win = win + reward\n",
    "            if reward < 0:\n",
    "                lose = lose -reward\n",
    "\n",
    "            # Apply the reinforcement strategy\n",
    "            loss = agent.reinforce(prev_state, state,  action, reward, game_over)\n",
    "\n",
    "        # Save as a mp4\n",
    "        if e % 10 == 0:\n",
    "            env.draw(prefix+str(e))\n",
    "\n",
    "        # Update stats\n",
    "        score += win-lose\n",
    "        print(\"Epoch {:03d}/{:03d} | Loss {:.4f} | Win/lose count {}/{} ({})\"\n",
    "              .format(e, epoch, loss, win, lose, win-lose))\n",
    "        agent.save(name_weights=prefix+'model.h5',name_model=prefix+'model.json')\n",
    "        \n",
    "class EnvironmentExploring(object):\n",
    "    def __init__(self, grid_size=10, max_time=500, temperature=0.1):\n",
    "        grid_size = grid_size+4\n",
    "        self.grid_size = grid_size\n",
    "        self.max_time = max_time\n",
    "        self.temperature = temperature\n",
    "\n",
    "        #board on which one plays\n",
    "        self.board = np.zeros((grid_size,grid_size))\n",
    "        self.position = np.zeros((grid_size,grid_size))\n",
    "        self.malus_position = np.zeros((grid_size,grid_size)) #define maluses when going to a previously visited position\n",
    "        # coordinate of the cat\n",
    "        self.x = 0\n",
    "        self.y = 1\n",
    "\n",
    "        # self time\n",
    "        self.t = 0\n",
    "\n",
    "        self.scale=16\n",
    "\n",
    "        self.to_draw = np.zeros((max_time+2, grid_size*self.scale, grid_size*self.scale, 3))\n",
    "\n",
    "\n",
    "    def draw(self,e):\n",
    "        skvideo.io.vwrite(str(e) + '.mp4', self.to_draw)\n",
    "\n",
    "    def get_frame(self,t):\n",
    "        b = np.zeros((self.grid_size,self.grid_size,3))+128\n",
    "        b[self.board>0,0] = 256\n",
    "        b[self.board < 0, 2] = 256\n",
    "        b[self.x,self.y,:]=256\n",
    "        b[-2:,:,:]=0\n",
    "        b[:,-2:,:]=0\n",
    "        b[:2,:,:]=0\n",
    "        b[:,:2,:]=0\n",
    "        \n",
    "        b =  cv2.resize(b, None, fx=self.scale, fy=self.scale, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        self.to_draw[t,:,:,:]=b\n",
    "\n",
    "\n",
    "    def act(self, action,train=False):\n",
    "        \"\"\"This function returns the new state, reward and decides if the\n",
    "        game ends.\"\"\"\n",
    "        #During the training phase going back to a position where the rat have already been before tends to decrease the \n",
    "        #total reward hence there is this train parameter that we have added (it tries to enforce the exploration)\n",
    "        self.get_frame(int(self.t))\n",
    "\n",
    "        self.position = np.zeros((self.grid_size, self.grid_size))\n",
    "\n",
    "        self.position[0:2,:]= -1\n",
    "        self.position[:,0:2] = -1\n",
    "        self.position[-2:, :] = -1\n",
    "        self.position[:, -2:] = -1\n",
    "\n",
    "        self.position[self.x, self.y] = 1\n",
    "        if action == 0:\n",
    "            if self.x == self.grid_size-3:\n",
    "                self.x = self.x-1\n",
    "            else:\n",
    "                self.x = self.x + 1\n",
    "        elif action == 1:\n",
    "            if self.x == 2:\n",
    "                self.x = self.x+1\n",
    "            else:\n",
    "                self.x = self.x-1\n",
    "        elif action == 2:\n",
    "            if self.y == self.grid_size - 3:\n",
    "                self.y = self.y - 1\n",
    "            else:\n",
    "                self.y = self.y + 1\n",
    "        elif action == 3:\n",
    "            if self.y == 2:\n",
    "                self.y = self.y + 1\n",
    "            else:\n",
    "                self.y = self.y - 1\n",
    "        else:\n",
    "            RuntimeError('Error: action not recognized')\n",
    "\n",
    "        self.t = self.t + 1     \n",
    "        \n",
    "        ## In Environment exploring:\n",
    "        # You will have to change n_state to 3 because you will use one more layer!\n",
    "        reward = 0\n",
    "        if train:\n",
    "            reward = -self.malus_position[self.x, self.y]\n",
    "        self.malus_position[self.x, self.y] = 0.1\n",
    "\n",
    "        reward = reward + self.board[self.x, self.y]\n",
    "        self.board[self.x, self.y] = 0\n",
    "        game_over = self.t > self.max_time\n",
    "        # 3 \"feature\" states instead of 2\n",
    "        state = np.concatenate((self.malus_position.reshape(self.grid_size, self.grid_size,1),\n",
    "                                        self.board.reshape(self.grid_size, self.grid_size,1),\n",
    "                                self.position.reshape(self.grid_size, self.grid_size,1)),axis=2)\n",
    "        state = state[self.x-2:self.x+3,self.y-2:self.y+3,:]\n",
    "\n",
    "        return state, reward, game_over\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"This function resets the game and returns the initial state\"\"\"\n",
    "        self.x = np.random.randint(3, self.grid_size-3, size=1)[0]\n",
    "        self.y = np.random.randint(3, self.grid_size-3, size=1)[0]\n",
    "\n",
    "\n",
    "        bonus = 0.5*np.random.binomial(1,self.temperature,size=self.grid_size**2)\n",
    "        bonus = bonus.reshape(self.grid_size,self.grid_size)\n",
    "\n",
    "        malus = -1.0*np.random.binomial(1,self.temperature,size=self.grid_size**2)\n",
    "        malus = malus.reshape(self.grid_size, self.grid_size)\n",
    "\n",
    "        self.to_draw = np.zeros((self.max_time+2, self.grid_size*self.scale, self.grid_size*self.scale, 3))\n",
    "\n",
    "\n",
    "        malus[bonus>0]=0\n",
    "\n",
    "        self.board = bonus + malus\n",
    "\n",
    "        self.position = np.zeros((self.grid_size, self.grid_size))\n",
    "        self.position[0:2,:]= -1\n",
    "        self.position[:,0:2] = -1\n",
    "        self.position[-2:, :] = -1\n",
    "        self.position[:,-2:] = -1\n",
    "        self.board[self.x,self.y] = 0\n",
    "        self.t = 0\n",
    "\n",
    "        self.malus_position = np.zeros((self.grid_size, self.grid_size))\n",
    "        #At the begining the malus_position array must be setted to zero\n",
    "        state = np.concatenate((self.malus_position.reshape(self.grid_size, self.grid_size,1),\n",
    "                                        self.board.reshape(self.grid_size, self.grid_size,1),\n",
    "                                self.position.reshape(self.grid_size, self.grid_size,1)),axis=2)\n",
    "\n",
    "        state = state[self.x - 2:self.x + 3, self.y - 2:self.y + 3, :]\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Epoch 000/010 | Loss 0.0206 | Win/lose count 4.5/24.10000000000006 (-19.60000000000006)\n",
      "Epoch 001/010 | Loss 0.9544 | Win/lose count 13.5/27.30000000000011 (-13.80000000000011)\n",
      "Epoch 002/010 | Loss 1.4232 | Win/lose count 5.5/23.100000000000055 (-17.600000000000055)\n",
      "Epoch 003/010 | Loss 1.2911 | Win/lose count 11.0/22.600000000000033 (-11.600000000000033)\n",
      "Epoch 004/010 | Loss 1.0153 | Win/lose count 4.0/21.600000000000044 (-17.600000000000044)\n",
      "Epoch 005/010 | Loss 1.1793 | Win/lose count 9.0/20.599999999999987 (-11.599999999999987)\n",
      "Epoch 006/010 | Loss 0.3208 | Win/lose count 8.0/28.40000000000012 (-20.40000000000012)\n",
      "Epoch 007/010 | Loss 0.1921 | Win/lose count 11.0/31.500000000000036 (-20.500000000000036)\n",
      "Epoch 008/010 | Loss 0.0563 | Win/lose count 10.0/21.300000000000022 (-11.300000000000022)\n",
      "Epoch 009/010 | Loss 1.3748 | Win/lose count 20.5/23.600000000000016 (-3.1000000000000156)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cnn_train_explore10.mp4'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-13139b538be5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDQN_CNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrain_explore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cnn_train_explore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mHTML\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdisplay_videos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cnn_train_explore10.mp4'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-28-8022f06c62a1>\u001b[0m in \u001b[0;36mdisplay_videos\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# display videos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdisplay_videos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mvideo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r+b'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mencoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase64\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb64encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     return '''<video alt=\"test\" controls>\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cnn_train_explore10.mp4'"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "env = EnvironmentExploring(grid_size=size, max_time=T, temperature=0.3)\n",
    "agent = DQN_CNN(size, lr=.1, epsilon = 0.1, memory_size=2000, batch_size = 32,n_state=3)\n",
    "train_explore(agent, env, epochs_train, prefix='cnn_train_explore')\n",
    "HTML(display_videos('cnn_train_explore10.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win/lose count 1.5/1.0. Average score (0.5)\n",
      "Win/lose count 1.5/0.0. Average score (1.0)\n",
      "Win/lose count 0/0.0. Average score (0.6666666666666666)\n",
      "Win/lose count 0.5/0.0. Average score (0.625)\n",
      "Win/lose count 1.5/1.0. Average score (0.6)\n",
      "Win/lose count 0/0.0. Average score (0.5)\n",
      "Win/lose count 0.5/2.0. Average score (0.21428571428571427)\n",
      "Win/lose count 0/1.0. Average score (0.0625)\n",
      "Win/lose count 1.5/2.0. Average score (0.0)\n",
      "Win/lose count 0/0.0. Average score (0.0)\n",
      "Final score: 0.0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cnn_test_explore10.mp4'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-598634141cce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Evaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cnn_test_explore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mHTML\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdisplay_videos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cnn_test_explore10.mp4'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-28-8022f06c62a1>\u001b[0m in \u001b[0;36mdisplay_videos\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# display videos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdisplay_videos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mvideo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r+b'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mencoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase64\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb64encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     return '''<video alt=\"test\" controls>\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cnn_test_explore10.mp4'"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "test(agent,env,epochs_test,prefix='cnn_test_explore')\n",
    "HTML(display_videos('cnn_test_explore10.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "__BONUS question__ Use the expert DQN from the previous question to generate some winning games. Train a model that mimicks its behavior. Compare the performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
